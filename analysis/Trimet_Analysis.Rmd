---
title: "TriMet Analysis"
author: "Karol Orozco, Corey Cassell, Justus Eaglesmith, Charles Hanks, & CorDarryl Hall"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(forcats)
library(stringr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(readxl)
```

# Exploratory Data Analysis of Trimet Security Data, 2017 - 2023

```{r, include=FALSE}
df <- read_csv('https://raw.githubusercontent.com/karolo89/capstone/main/data/trimet_complete.csv')
summary(df)

unique(df$subtype_desc)

df %>% group_by(subtype_desc) %>% count() %>% arrange(desc(n))
```

## Examining data with subtype as "Other": 

```{r}
# First, calculate the table (count) of df$subtype_desc
subtype_counts <- table(df$subtype_desc)

# Then, sort it in decreasing order
subtype_counts <- sort(subtype_counts, decreasing = TRUE)

# Next, convert it to a dataframe for easy manipulation with ggplot2
subtype_counts_df <- as.data.frame.table(subtype_counts)

# Finally, create the bar plot
ggplot(subtype_counts_df, aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "Subtype Description", y = "Count") +
  ggtitle("Counts of each subtype description")

```

Ridiculous amount of missing data, lets see if we can find trends with the missing data.

```{r}
df$is_other <- ifelse(df$subtype_desc == '[Other]', TRUE, FALSE)

# Filter rows where is_other is TRUE and select only categorical variables
df_other <- df %>%
  filter(is_other == TRUE) %>%
  select(where(is.character), -comments, -incident_date, -incident_id)
# Create a list of tables, each providing a proportion of each category for each variable
list_of_tables <- lapply(df_other, function(x) prop.table(table(x, useNA = "ifany")))
```

```{r}
# Iterate over each item in the list
for(i in seq_along(list_of_tables)) {
  # Print the variable name
  cat("\nProportions for variable", names(list_of_tables[i]), ":\n")
  
  # Print the table
  print(list_of_tables[[i]])
}

```

```{r}
library(zoo)

# Calculate the proportion of 'Other' incidents per day
df_other_over_time <- df %>%
  group_by(incident_date) %>%
  summarize(proportion_other = mean(is_other, na.rm = TRUE))

# Add a 7-day rolling average to the data
df_other_over_time <- df_other_over_time %>%
  mutate(rolling_avg = rollmean(proportion_other, 7, fill = NA, align = "right"))

# Plot
ggplot(df_other_over_time, aes(x = incident_date)) +
  geom_smooth(aes(y = proportion_other), alpha = 0.3) +
  geom_smooth(aes(y = rolling_avg), color = "blue") +
  labs(x = "Date", y = "Proportion of 'Other'", 
       title = "7-Day Rolling Average of 'Other' Incidents Over Time")
```

```{r}
# Filter rows where is_other is TRUE, count occurrences by location, and arrange in descending order
df_other_by_location <- df %>%
  filter(is_other) %>%
  count(location) %>%
  arrange(desc(n))

# Select the top locations (e.g., top 10)
top_locations <- df_other_by_location %>%
  top_n(10)

# Plot
ggplot(top_locations, aes(x = reorder(location, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Location", y = "Count", 
       title = "Top occurrences of 'Other' by location")

```

# Features of Other:
- type code only = 'ASST'
- Division only = 'R'
- Type Description is only 'Security'
- No subtype codes
- Appears that other incidents increased in 2022 but are trending downwards 2023
- Gateway TC has the most out of all locations
- Incident Subtype is the only subtype_t


```{r, include = FALSE}
# Load the lubridate package
library(lubridate)

# Convert the incident_date to a date-time object and extract the year
df$year <- year(as.POSIXct(df$incident_date, format="%m/%d/%Y %H:%M"))

# Now calculate counts and average for each year
df_year_counts <- df %>%
  count(year) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

# Convert the incident_date to a date-time object and extract the month
df$month <- month(as.POSIXct(df$incident_date, format="%m/%d/%Y %H:%M"), label=TRUE)

# Now calculate counts and average for each month
df_month_counts <- df %>%
  count(month) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))
# Convert the incident_date to a date-time object and extract the day
df$day <- day(as.POSIXct(df$incident_date, format="%m/%d/%Y %H:%M"))

# Now calculate counts and average for each day
df_day_counts <- df %>%
  count(day) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

# Convert the incident_date to a date-time object and extract the hour
df$hour <- hour(as.POSIXct(df$incident_date, format="%m/%d/%Y %H:%M"))

# Now calculate counts and average for each hour
df_hour_counts <- df %>%
  count(hour) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))


```



# Yearly Counts

```{r}

# Plot yearly counts
ggplot(df_year_counts, aes(x = year, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Year", y = "Count", fill = "", title = "Number of Security Incidents Per Year")
```

The incidents are increasing annually so it is important to get a handle on this. Especially considering 2022 had tremendously more results than 2021. A better plot for this would be using a timeseries plot. 

# Monthly Counts

```{r}
# Calculate counts and average
df_month_counts <- df %>%
  count(month) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

ggplot(df_month_counts, aes(x = month, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(y = "Count", fill = "")

```

We should focus on the months Dec - May as those have the most incidents and are above the average. It would be valuable to understand what could be causing the increase in these specific months as well.



# Daily Counts

```{r}
# Calculate counts and average for each day
df_day_counts <- df %>%
  count(day) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

# daily counts
ggplot(df_day_counts, aes(x = day, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Day of Month", y = "Count", fill = "")


```
day_



Days are a little sporatic but it looks like earlier in the month and later in the months there are more incidents, first and last week of the month specifically.

# Hourly Incidents

```{r}
#Hourly incidents
library(lubridate)

# Create a new hour field
df$hour <- hour(as.POSIXct(df$incident_date, format="%m/%d/%Y %H:%M"))

# Calculate counts and average for each hour
df_hour_counts <- df %>%
  count(hour) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

# Plot hourly counts
ggplot(df_hour_counts, aes(x = hour, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Hour of the Day", y = "Count", fill = "")


```

Clearly it shows that many incidents occur at hour 6-12 and 3-12a. Makes sense as most people are commuting or using the transportation services in the morning before work/school and after work/school.


```{r}
#Parsing text to extract common themes amoungst reported incidents
comments_corpus <- Corpus(VectorSource(df$comments))

comments_corpus <- tm_map(comments_corpus, content_transformer(tolower))
comments_corpus <- tm_map(comments_corpus, removePunctuation)
comments_corpus <- tm_map(comments_corpus, removeNumbers)
comments_corpus <- tm_map(comments_corpus, removeWords, stopwords("english"))
comments_corpus <- tm_map(comments_corpus, stemDocument)

```

```{r}
library(tidytext)
# Converting the text to lower case
df$comments <- tolower(df$comments)

# Removing punctuation, numbers, stop words and white spaces
df$comments <- removePunctuation(df$comments)
df$comments <- removeNumbers(df$comments)
df$comments <- removeWords(df$comments, stopwords("english"))
df$comments <- stripWhitespace(df$comments)

# Tokenizing the words
df_tokens <- df %>%
  unnest_tokens(word, comments)

# Counting the frequency of each word
df_word_counts <- df_tokens %>%
  count(word, sort = TRUE)

# Filtering out words with less than 3 characters
df_word_counts <- df_word_counts[nchar(df_word_counts$word) > 2, ]

# Displaying the top 10 words
top_10_words <- df_word_counts %>%
  top_n(10) %>%
  mutate(word = reorder(word, n))

ggplot(top_10_words) +
  geom_col(aes(x = word, y = n, fill = n)) +
  labs(x = "Term", y = "Frequency", title = "Top 10 Terms in Comments") +
  coord_flip()
```
Removing common words: 
```{r}
dtm <- df %>%
unnest_tokens(word, comments) %>%
anti_join(stop_words) %>% # get rid of stop words
filter(!(word %in% c("train","notification", "police", "reports","cleared","trains","time","ave","due","check","operator","supervisor", "incident", "called", "reported", "report", "arrived", "unit", "platform", "person", "requested", "individual", "stated", "delay", "requesting"))) %>% #removing common words 
count(incident_id, word) %>%
group_by(incident_id) %>%
mutate(freq = n/sum(n)) %>%
mutate(exists = (n>0)) %>%
ungroup %>%
group_by(word) %>%
mutate(total = sum(n))


dtm %>% 
count(word, sort = TRUE) %>% 
  filter(n > 1000) %>%
    ggplot(aes(x = n , y= reorder(word,n))) + geom_col() + labs(y = NULL) + labs(title = "Most Common Words 2")
```

Incidents occur mostly on the train it appears, however we should look at the next most common phrases or nouns/adjectives to get a better understanding.


# Analyzing Security Incidents at Night 

What type of security incidents occurs most frequently as night ? This will require subcategory per each incident. At present in this dataset, 82% of incidents have subcategory of 'other'. We could generate subcategories through text analysis of `comments` column.

```{r}
df %>% group_by(subtype_desc) %>% count() 
14591/nrow(df)

```
Winter : December - February 
Spring : April - June 
Summer : July - August 
Fall : September - November 

```{r}
df = df %>% mutate(season = factor(case_when(
                          month %in% c('Dec','Jan','Feb') ~ 'Winter', 
                          month %in% c('Mar','Apr', 'May') ~ 'Spring', 
                          month %in% c('Jun', 'Jul', 'Aug') ~ 'Summer', 
                          month %in% c('Sep', 'Oct','Nov') ~ 'Fall')),
              night_time = factor(ifelse(hour >= 20 | hour <= 4, 1, 0)))

```


Approximating nighttime as between the hours of 20:00 and 05:00, how many incidents occur at night vs during day?  
```{r}
df %>% group_by(night_time, season) %>% count()  %>% 
            ggplot(aes(x = season, y = n, fill = night_time)) + geom_col() + facet_grid(~ night_time) + 
            scale_discrete_manual(aesthetics = c("fill"), values = c("#FF7F7F", "#7F7FFF")) + labs(title = "# of Incidents, Day vs Night Per Season")
 
```
We see a decrease in night time incidents during summer time. perhaps this suggests that more incidents occur when more people are using TriMet services due to weather. For example, people seeking shelter in max trains due to cold and wet conditions. 


What is spread of night incidents in one year?
```{r}
ggplot(df %>% filter(night_time == 1) %>% 
  count(month) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average")), aes(x = month, y = n, fill = color)) + 
        geom_bar(stat = "identity", show.legend = FALSE) +
        geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
        scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
        labs(y = "Count", fill = "") + labs(title = "Night Incidents by Month")
```
In summer and fall we see a decrease in incidents at night. 

```{r}
df %>% group_by(year, night_time) %>% count() %>% ggplot(aes(x = year, y = n, fill = night_time)) + geom_col() 
```

The number of incidents at night is proportional to total number of incidents per year. 

Where are the most incidents occurring at night? 

```{r}
df %>% filter(night_time == 1) %>%  
  group_by(location, type_desc ) %>% 
    count() %>% 
      arrange(desc(n))

```
Elmonica is where the MAX trains are stored and serviced - most trains in the morning start here. It would make sense that security personnel are reporting from here. 
Gateway is a hot spot for all types of activity. `Cleveland Avenue` we assume is in reference to MAX station in Gresham (final stop for the Blue Line) Cleveland is mentioned as a place where 'sleepers' are found. 


In 2023, have we seen a downward trend in incidents at night given the increased presence of security personnel starting in March 2023? 

```{r}
df = df %>% mutate(y_m_d = date(as.POSIXct(incident_date, format="%m/%d/%Y %H:%M"))) # adding date only column 

df %>% filter(year == 2023) %>% group_by(y_m_d, night_time) %>% count() %>%  ggplot(aes(x = y_m_d, y = n, fill = night_time))  + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE) + 
  facet_grid(~night_time)


```

What if we compare the same time interval, one year ago? 
```{r}
df %>% filter(year %in% c(2022,2023))%>%
  filter((y_m_d >= "2022-01-01" & y_m_d <= "2022-05-16") | y_m_d >= "2023-01-01")%>% 
  group_by(y_m_d,year, night_time) %>% count() %>%  ggplot(aes(x = y_m_d, y = n, color = night_time))  + geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, color = "black") + 
  facet_grid(night_time~year) + 
  theme_minimal() + labs(title = "Jan - May, 2022 vs 2023")

```
Nighttime incidents were trending upward in Jan - May 2022, and now they are trending downward Jan - May 2023. 


# PPB Crime Data: 

How does trimet's security incident increase in 2022 compare with PPB crime data ? We want to rule out the reason for this increase in 2022 as just an increase in reporting / security personel. 

```{r}
c_17_23 = read_csv('https://raw.githubusercontent.com/karolo89/capstone/main/data/ppb_2017_2023.csv')
```

```{r}
#formatting date and time columns 
c_17_23 = c_17_23 %>% mutate(OccurDate = as.POSIXct(OccurDate, format = "%m/%d/%Y"), 
                             ReportDate = as.POSIXct(ReportDate, format = "%m/%d/%Y"),
                             OccurTime = hms::as.hms(as.POSIXct(OccurTime, format = "%H%M"))) %>% relocate(OccurDate, OccurTime, ReportDate)

#coercing char cols into factors 
cols2factor = c("CrimeAgainst", "Neighborhood", "OffenseCategory", "OffenseType")
c_17_23[cols2factor] = lapply(c_17_23[cols2factor], factor)

#parsing year, month, hour 
c_17_23 = c_17_23 %>% mutate(year = factor(year(OccurDate)), 
                             month = factor(month(OccurDate)), 
                             hour = hour(OccurTime))

```

Counting events by year, month, hour
```{r}
#did 2023 also have a signifcant increase in incidents report to PPB? 
c_17_23 %>% 
  filter(OccurDate >= "2017-01-01" & OccurDate < "2023-01-01") %>%
      group_by(year) %>% 
        count() %>% 
           mutate(is_2022 = year == 2022) %>% 
            ggplot(aes(x = year, y = n)) + geom_col(aes(fill = is_2022)) + geom_hline(aes(yintercept = mean(n)), linetype = "dashed", color = "black") + labs(title = "Number of incidents report to PPB per year")

#do we see the same trend where more incidents occur Dec - May? 
c_17_23 %>% 
  filter(OccurDate >= "2017-01-01" & OccurDate < "2023-01-01") %>% 
    group_by(month) %>% 
      count() %>% ggplot(aes(x = month, y = n)) + geom_col() 

# nope 

# at what hour do the most incidents occur? 
c_17_23 %>% 
  filter(OccurDate >= "2017-01-01" & OccurDate < "2023-01-01") %>% 
    group_by(hour) %>% 
      count() %>% 
        mutate(is_midnight = hour == 0) %>% ggplot(aes(x = hour, y = n)) + geom_col(aes(fill = is_midnight)) 


  
```

What were the most common type of incidents in 2022 vs 2021  ?

```{r}
c_17_23 %>% filter(year %in% c(2020:2022)) %>%
  group_by(OffenseType, year) %>% count() %>% 
  filter( n > 100) %>% 
    ggplot(aes(x = fct_reorder(OffenseType,n), y = n)) + geom_col() + coord_flip() + labs(title = "Most Common type of crimes in 2022") + facet_grid(~year)

#huge increase in vehicle theft and vandalism 

```

# TEXT ANALYSIS

The goal of this section is to familiarize ourselves with the jargon and terminology of security incident recordings, as well as parse key words from comments section for later feature engineering.

### What is a ROW trespasser?
"ROW trespasser" refers to an individual who enters or occupies the right-of-way (ROW) of the transit system without authorization or permission. The right-of-way refers to the designated area or space where the transit system operates, such as railway tracks or bus lanes. Trespassing in the ROW can pose significant safety risks, as it puts the individual and others, including transit operators and passengers, in danger of accidents or collisions.

```{r}
df %>% filter(subtype_desc == 'ROW Trespasser')
```
It appears that ROW trespasser refers to people on foot, usually on tracks or in restricted areas. 

### TOW
refers to a tow called to move a vehicle parked at trimet parking lots over time, or an abandoned vehicle
```{r}
df %>% filter(str_detect(subtype_desc,'Tow'))

```

Other key words to investigate from comments: possible dummy columns
'DK male' 
'smoking'
'bother' 
'took swing' 
'hit' 
'struck' 
'assault-' 
'throw' 
'grab'
'altercation'
'hate speech' 
'police'
'shooting' 
'taken to' 
'lying' 
'intoxicated' 
'urin-' 
'shut down' 
'threat-' 
'physic-' 
'menacing' 
'red knob' : needs to be reset by operator
'refus-'
'face'
'medical'
'fled' 
'injur-' 
'steel pole' 
'spit', 
'cuss', 
'data packs' 
'late'
'shov-' 
'slam-' 
'nefarious' 
'erratic' 
'sleep-'
'hurt-' 
'delay' 
'belige-' ( a few alternate spellings)
'teenager' 
'punch' 
'going off' 
'yell-' 
'uncooperative' 
'aggressive' 
'cigarette'
'ass'
'unwanted'
'mental breakdown' 
'knife' 
'kill' 
'camp'
'emergency mushroom'
'pound' 
'suicid-' 
'fire' 
'firearm' 
'harras'
'interdicted'
'bio' 
'genitals'
'blood'

```{r}
unique(df$type_code)
```

Is 'nefarious' a technical term in security? Or is this a favored word by a particular security guard? 

```{r}
df %>% filter(str_detect(comments,'[Nn]efarious'))
```

Drug & alcohol related incidents: 
```{r}
drug_words = c('[Dd]rug','[Ss]hooting up','[Pp]araphenelia','[Bg]aggie','[Nn]eedle','[Hh]igh','[Cc]rack', '[Hh]erion','[Mm]eth', '[Pp]ipe', '[Ii]ntoxicat', '[Cc]ocaine', '[Hh]ypodermic','[Ff]ume', '[Dd]ose','[Ff]entanyl','[Nn]arcotic','[Ii]nhalent', '[Cc]annabis','[Mm]arijuana','[Ff]oil','[Bb]eer','[Aa]lcohol','[Dd]runk')

df %>% filter(str_detect(comments, pattern = paste(drug_words, collapse = "|")))

```

sexual harassment & assault: 
```{r}
sex_words = c('[Ll]ewd','[Ss]ex','[Mm]asturbat','[Ii]nappropriate','[Gg]enital','[Rr]ape','[Hh]arass')

df %>% filter(str_detect(comments, pattern = paste(sex_words, collapse = "|")))
```

Violent acts 
```{r}
violent_words <- c('[Ss]wing', '[Hh]it', '[Ss]truck', '[Aa]ssault', '[Tt]hrow', '[Gg]rab',
           '[Tt]hreat', '[Pp]hysic', '[Mm]enac','[Ff]ace','[Ff]led', '[Ii]njur', '[Ss]teel pole', '[Ss]pit','[Ss]hov', '[Ss]lam','[Hh]urt', '[Bb]elig',
           '[Pp]unch', '[Gg]oing off', '[Yy]ell','[Aa]ggressive','[Gg]un', '[Kk]nife', '[Kk]ill',
           '[Pp]ound', '[Ss]uicid', '[Ff]ire', '[Ff]irearm', '[Hh]arras', '[Bb]lood')

df %>% filter(str_detect(comments, pattern = paste(violent_words, collapse = "|")))

```
hate speech 
```{r}
df %>% filter(str_detect(comments, "hate speech"))
```
bio fluids 
```{r}
bio_words = c('[Uu]rin','[Bb]lood','[Bb]leed', '[Vv]omit','[Ss]ubstance', '[Bb]io harzard')
df %>% filter(str_detect(comments, pattern = paste(bio_words, collapse = "|")))
```

### TVM = "ticket vending machine" 
People try to break into TVMs to steal tickets / money. 
```{r}
df %>% filter(str_detect(subtype_desc,'TVM'))
```









# WRANGLING LOCATION DATA 

Using sf package to load TriMet transit center shapefile: 

```{r}
library(sf)

shape = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_tran_cen', 
                layer = 'tm_tran_cen')

shape %>% head()

#does it plot? 
shape %>% ggplot() + geom_sf() + theme_bw() 
```
#loading spatial data from package maps
```{r}
library(maps)

tm_counties = map_data("county","oregon") %>% 
  select(lon = long, lat, group, id = subregion) %>% filter(id %in% c("multnomah", "clackamas", "washington"))

ggplot(tm_counties, aes(lon, lat, group = group)) + 
  geom_polygon(fill = "white", colour = "grey50") + 
  coord_quickmap()
```
```{r}
library(tigris)
options(tigris_use_cache = TRUE)

this.year = 2020

mult_tracts = tracts(state = 'OR', county = "Multnomah", cb = T, year = this.year)
clack_tracts = tracts(state = 'OR', county = "Clackamas", cb = T, year = this.year)
wash_tracts = tracts(state = 'OR', county = "Washington", cb = T, year = this.year)

tri_county_tracts = bind_rows(mult_tracts,clack_tracts,wash_tracts)


plot(tri_county_tracts)
```

Plot of Trimet Transit Centers Across 3 Counties 
```{r}
ggplot() + geom_sf(data = tri_county_tracts) + 
          geom_sf(data = shape, color = "dodgerblue", size = 5) + 
          labs(title =  "TriMet Transit Centers") +
          #geom_sf_text(data = shape, aes(label = name), size = 2, nudge_y = -.0002) + 
          coord_sf(xlim=c(-123.1, -122.4), ylim = c(45.3,45.6)) thi



```
Renaming transit center names according to shapefile:
```{r}
#subsetting dataset with only transit centers 
tc = df %>% filter(str_detect(location, "Tc")) 

unique(tc$location)
unique(shape$name)

tc2 = tc %>% mutate(location = 
                case_when(
                  grepl("Rose",location) ~ "Rose Quarter Transit Center", 
                  grepl("Beaverton",location) ~ "Beaverton Transit Center",
                  grepl("Gateway",location) ~ "Gateway/NE 99th Ave Transit Center",
                  grepl("Clack",location) ~ "Clackamas Town Center Transit Center",
                  grepl("Sunset",location) ~ "Sunset Transit Center",
                  grepl("Willow",location) ~ "Willow Creek/SW 185th Ave Transit Center",
                  grepl("Gresham",location) ~ "Gresham Central Transit Center",
                  grepl("Hillsboro",location) ~ "Hillsboro Central/SE 3rd Ave Transit Center",
                  grepl("Lombard Tc",location) ~ "N Lombard Transit Center",
                  grepl("Tigard",location) ~ "Tigard Transit Center",
                  grepl("Hollywood",location) ~ "Hollywood/NE 42nd Ave Transit Center",
                  grepl("Rose",location) ~ "Rose Quarter Transit Center",
                  grepl("Sunset",location) ~ "Sunset Transit Center"))
                  
unique(tc2$location)

```

### Converting shapefile data to lon + lat: 
[StackExchange](https://gis.stackexchange.com/questions/296170/r-shapefile-transform-longitude-latitude-coordinates) on converting shapefile to WGS84 coordinates using `sf` package

```{r}
tc_coords = st_transform(shape,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>% st_coordinates()

shape = bind_cols(shape, tc_coords) %>% rename(lat = Y, lon = X)
```

converting shapefile to coords for routes_and_stops shapefile 
```{r}
rs_coords = st_transform(shape2,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

rs_shape = bind_cols(shape2, rs_coords) %>% rename(lat = Y, lon = X)
rs_shape = rs_shape %>% mutate(location = stop_name) %>% relocate(location) %>% select(-stop_name)

```

The process so far is: 
Change location name in `df` to match the location name in shapefiles. Join shapefile data with `df` data on `location` key.


This is a test, subsetting df to grab only transit centers: 
```{r}
#renaming name to location so that they share col name to join on 
shape = shape %>% mutate(location = name) %>% relocate(location) %>% select(-name) # for some reason rename() was throwing an error, so this is a work-around to change name col to location for joining

tc2 = tc2 %>% left_join(shape, by = "location")
#Okay! We have now have 3,608 security incidents that have complete special data.

#joining shape to original dataset 
```

```{r}
#first I need to change transit centers names in df 

df = df %>% mutate(location = 
                case_when(
                  grepl("Rose",location) ~ "Rose Quarter Transit Center", 
                  grepl("Beaverton",location) ~ "Beaverton Transit Center",
                  grepl("Gateway",location) ~ "Gateway/NE 99th Ave Transit Center",
                  grepl("Clack",location) ~ "Clackamas Town Center Transit Center",
                  grepl("Sunset",location) ~ "Sunset Transit Center",
                  grepl("Willow",location) ~ "Willow Creek/SW 185th Ave Transit Center",
                  grepl("Gresham",location) ~ "Gresham Central Transit Center",
                  grepl("Hillsboro",location) ~ "Hillsboro Central/SE 3rd Ave Transit Center",
                  grepl("Lombard Tc",location) ~ "N Lombard Transit Center",
                  grepl("Tigard",location) ~ "Tigard Transit Center",
                  grepl("Hollywood",location) ~ "Hollywood/NE 42nd Ave Transit Center",
                  grepl("Rose",location) ~ "Rose Quarter Transit Center",
                  grepl("Sunset",location) ~ "Sunset Transit Center",
                  TRUE ~ location)
                )

#now need to join spatial data to these 

num_per_tc = df %>% filter(str_detect(location, "Transit Center")) %>% group_by(location) %>% count()  

df2 = df %>% left_join(shape, by = "location") # ok this is weird, every row got geometry point shape data

df2 = df2 %>% mutate(geometry = ifelse(str_detect(location, "Transit Center"), st_as_sf(geometry), NA)) #removing the duplicated shapefile point from geometry 
```

With transit centers now wrangled, what are the next big locations in the dataset ? 
```{r}
df %>% group_by(location) %>% count() %>% arrange(desc(n))
```

Elmonica / SW 170th Ave is a light rail station on the Max Blue line 
```{r}
df %>% filter(str_detect(location,"Elmonica"))

#So there is the blue max station, and then the neighboring yard. Let's group these events together -- it's within the same geographic area. 

df = df %>% mutate(location = ifelse(str_detect(location, "Elmonica"),"Elmonica/Sw 170th", location)) # okay now 552 incidents are in the Same elmonica 


#now let's find the shapefile / coords for this location 

rs_shape %>% filter(str_detect(stop_name, "Elmonica")) # "Elmonica/SW 170th Ave MAX Station"


#changing df location to match shapefile stop_name 

df = df %>% mutate(location = ifelse(location == "Elmonica/Sw 170th","Elmonica/SW 170th Ave MAX Station", location))

# subsetting elmonica spatial data 
elmo = rs_shape %>% filter(str_detect(stop_name, "Elmonica")) %>% filter(stop_seq == 1550) %>% mutate(location = 'stop_name')

df %>% filter(location == "Elmonica/SW 170th Ave MAX Station")

df %>% left_join(elmo, by = "location")

```



```{r}
tc_2022_num_incidents = tc2 %>% filter(year == 2022) %>% 
  group_by(location) %>% count() %>% 
    left_join(tc2 %>% select(location, lon, lat), by = "location") %>% group_by(location, n) %>% unique()

tc_2022_num_incidents %>% 
  ggplot() + 
  geom_sf(data = tri_county_tracts) +
  geom_point(mapping = aes(x = lon, y = lat, size = n, color = location)) +
  labs(title =  "Security Incident Count in 2022 at TriMet Transit Centers") +
  #geom_sf_label(data = shape, aes(label = location), size = 2, nudge_y = -.04) +
  coord_sf(xlim=c(-123.1, -122.4), ylim = c(45.4,45.6)) +
  theme(legend.position = "top")
```

```{r}
library(plotly)

tc_2017_2023 = tc2 %>%
  group_by(location, year) %>% count() %>% 
    left_join(tc2 %>% select(location, lon, lat), by = "location") %>% group_by(location, year, n) %>% unique()

plot <- tc_2017_2023 %>%
  ggplot() +
  geom_col(aes(x = location, y = n, fill = factor(year))) +
  coord_flip()

ggplotly(plot, dynamicTicks = TRUE)
```

Exploring other shapes files 

```{r}
shape2 = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_route_stops', 
                layer = 'tm_route_stops')

head(shape2)

shape2_coords = st_transform(shape2,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>% st_coordinates()

shape2 = bind_cols(shape2, shape2_coords) %>% rename(lat = Y, lon = X)
```


```{r}
routes = ggplot() + geom_sf(data = tri_county_tracts) + 
          geom_sf(data = shape2, aes(color = factor(rte)), linetype = "11") + 
          labs(title =  "TriMet Routes & Stops") +
          #geom_sf_text(data = shape, aes(label = name), size = 2, nudge_y = -.0002) + 
          coord_sf(xlim=c(-123.1, -122.4), ylim = c(45.3,45.6)) + 
          theme(legend.position = "none")

routes
```




#######

New Data file from Trimet: `trimet_2010_2023.csv` 

Rexamining date + time trends using larger dataset 
```{r}
library(lubridate)

df2 = read_csv('/Users/chanks/workspace/capstone/data/trimet_2010_2023.csv')

unique(df2$INCIDENT_SUBTYPE_CODE_LIST) # MUSHROOM ?? 

df2 = df2 %>% mutate(date = as.POSIXct(date, format="%m/%d/%Y"))

df2 = df2 %>% mutate(year = year(date), 
                     month = month(date, label = TRUE), 
                     day = day(date), 
                     wday = wday(date, label = TRUE), 
                     hour = hour(time))

# Now calculate counts and average for each year
df_year_counts2 <- df2 %>%
  count(year) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

ggplot(df_year_counts2, aes(x = year, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Year", y = "Count", fill = "", title = "Number of Security Incidents Per Year")


# Now calculate counts and average for each month
df_month_counts2 <- df2 %>%
  count(month) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

ggplot(df_month_counts2, aes(x = month, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(y = "Count", fill = "")

# Now calculate counts and average for each day
df_day_counts2 <- df2 %>%
  count(day) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))

ggplot(df_day_counts2, aes(x = day, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Day of Month", y = "Count", fill = "")

# Now calculate counts and average for each hour
df_hour_counts2 <- df2 %>%
  count(hour) %>%
  mutate(avg = mean(n),
         color = ifelse(n > avg, "Above Average", "Below Average"))


# Plot hourly counts
ggplot(df_hour_counts2, aes(x = hour, y = n, fill = color)) + 
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_hline(aes(yintercept = avg), linetype = "dashed", color = "black") +
  scale_fill_manual(values = c("Above Average" = "#FF7F7F", "Below Average" = "#7F7FFF")) +
  theme_minimal() +
  labs(x = "Hour of the Day", y = "Count", fill = "")

```

```{r}
df2 = df2 %>% rename_all(funs(tolower(.))) # I like lower case columns 

df2 = df2 %>% select(-incident_begin_date,-division_code, -x_coordinate,-y_coordinate,-loc_x,-loc_y) # removing non-pertinent columns 

#looks like we have some duplicate incidents 

df2 %>% group_by(comments) %>% count() %>% arrange(desc(n))
df2= df2[!duplicated(df2$incident_id),] #keeping only 1 incident per incident_id 

#quick removal of some unwanted text in comments 
patterns = c("<Notification>","\r","\n")
df2 = df2 %>% mutate(comments = gsub(paste(patterns, collapse = "|"), "", comments))

write_csv(df2, file = '/Users/chanks/workspace/capstone/data/trimet_2010_2023_v2.csv')

unique(df2$incident_type_code_list)

df2 %>% group_by(incident_subtype_code_list) %>% count()
```

