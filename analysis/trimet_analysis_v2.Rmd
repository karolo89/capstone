---
title: "Trimet Security Analysis"
author: Karol Orozco, Corey Cassell, Justus Eaglesmith, Charles Hanks, & CorDarryl Hall
output: html_document
date: "2023-06-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(lubridate)
library(gridExtra)
library(forcats)
library(stringr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(tigris)
library(sf)
library(skimr)
```

## DATA CLEANING

```{r}
#this is the raw data provided by TriMet, through Karol, on Jun 24 2023
df = read_csv(
  'https://raw.githubusercontent.com/karolo89/capstone/main/data/trimet_2010_2023.csv') %>%
  rename_all(funs(tolower(.)))   %>% # I like lower case columns
  select(-incident_begin_date, # removing non-pertinent columns 
         -division_code,
         -x_coordinate,
         -y_coordinate,
         -loc_x,
         -loc_y) 

#adding date + time columns:
df = df %>% mutate(date = as.POSIXct(date, format = "%m/%d/%Y")) %>% 
  mutate(
    year = year(date),
    month = month(date, label = TRUE),
    day = day(date),
    wday = wday(date, label = TRUE),
    hour = hour(time))

#looks like we have some duplicate incidents 
df= df[!duplicated(df$incident_id),]

#quick scrub of some unwanted tags in comments 
patterns = c("<Notification>","\r","\n")
df = df %>% mutate(comments = gsub(paste(patterns, collapse = "|"), "", comments))

#looks like several columns are incomplete, may have to make some sacrifices for the sake of clean, workable data. 
#For example, I think for our purposes we can drop garage, primary_vehicle_flag, and train.

df = df %>% select(-garage, -primary_vehicle_flag, -train)

df %>% filter(is.na(route_number)) # note that 18,422 of the incidents do not have a route number assigned, this may be consideration later in analysis when drilling down to incidents per route 

# every incident has a location id! This is good. 
df %>% filter(!is.na(location_id)) 

skim(df) #also note: 524 rows are missing comments, 4149 rows are missing location names (this coudl be rectified as we have location_id)


```

## LOCATION DATA ANALYSIS 

### Loading TriMet Shapefiles

routes & stops (primary shapefile)
```{r}
#loading trimet's 'routes & stops' shapfiles, courtesy of Trimet's developer resources: <https://developer.trimet.org/>
shape_rs = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_route_stops', 
                layer = 'tm_route_stops') #loading routes and stops shapefile data 

# confirming that shapefile `stop_id` is the same as security dataset's `location_id` 
shape_rs %>% filter(str_detect(stop_name,"Hollywood")) # oneHollywood TC id is 10871

```

Additional shapefiles
```{r}
shape_rail_stops = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_rail_stops', 
                layer = 'tm_rail_stops') 

coords_rail_stops = st_transform(shape_rail_stops,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_rail_stops = bind_cols(shape_rail_stops, coords_rail_stops) %>% rename(lat = Y, lon = X)

#nope, this does not have the data I want 


shape_parkride = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_parkride', 
                layer = 'tm_parkride') 

coords_parkride = st_transform(shape_parkride,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_parkride = bind_cols(shape_parkride, coords_parkride) %>% rename(lat = Y, lon = X)

#nice to have, but this does not have location_id 

shape_tc = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_tran_cen', 
                layer = 'tm_tran_cen')
```


Preparing tract data
```{r}
library(tigris)
options(tigris_use_cache = TRUE)

this.year = 2020

mult_tracts = tracts(state = 'OR', county = "Multnomah", cb = T, year = this.year)
clack_tracts = tracts(state = 'OR', county = "Clackamas", cb = T, year = this.year)
wash_tracts = tracts(state = 'OR', county = "Washington", cb = T, year = this.year)

tri_county_tracts = bind_rows(mult_tracts,clack_tracts,wash_tracts)

plot(tri_county_tracts)
```

Adding WGS84 coordinates to routes and stops shapefile 
```{r}
coords_rs = st_transform(shape_rs,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_rs = bind_cols(shape_rs, coords_rs) %>% mutate(lat = Y, lon = X) %>% select(-Y,-X) #rename(lat = Y, lon = X)
# I have confirmed on https://www.gps-coordinates.net/ that these lat + lon coords correspond to stop / intersection 

length(unique(shape_rs$stop_id)) #6430 distinct stop_ids 
length(shape_rs$stop_id) # 8539 total stop_ids listed, this is likely because multiple routes share the same stop

#shape_rs %>% group_by(stop_id) %>% count() %>% arrange(desc(n))
# ther are 21 routes that use stop id 13248

shape_rs = st_transform(shape_rs, crs = st_crs(tri_county_tracts)) #joining tract information to shape_rs (for mapping later)
shape_rs = st_join(shape_rs,tri_county_tracts)  

#I also want the tract multipolygon spatial data
class(tri_county_tracts) # this is a sf object, I need to cast it as data frame to do non-spatial join 

shape_rs = shape_rs %>% left_join(tri_county_tracts %>% select(TRACTCE,geometry) %>% as.data.frame(), by = "TRACTCE")


shape_rs = shape_rs %>% rename_all(funs(tolower(.))) 
```


Prepping route and stops shapefile to join 
```{r}
#changing stop_id to location_id so that dataframes may join on common key (rename() throws an error that I haven't resolved)
shape_rs = shape_rs %>% mutate(location_id = stop_id) %>% select(-stop_id) %>% relocate(location_id, .after = stop_name)

#selecting only the relevant columns to join: 
#shape_rs = shape_rs %>% select(stop_name, location_id, zipcode, frequent, geometry, lon, lat) 

#we only want one location_id (formerly stop_id) in this shape file to avoid a many-to-many join ("For stops served by multiple lines there are multiple records in this dataset.")
shape_rs_distinct = shape_rs %>% distinct(location_id, .keep_all = TRUE)

shape_rs_distinct %>% filter(location_id == 13248) # confirming that this clackamas tc stop has only one row



```

### JOIN of df with shape_rs_distinct
```{r}
df2 = df %>% left_join(shape_rs_distinct %>% select(-rte,-dir,-rte_desc,-dir_desc,-stop_seq), by = "location_id")

```

The `shape_rs` table is missing some locations that are in the `df2`....I will hunt these down.
```{r}
#for example, this gateway transit stop 
df2 %>% filter(location_id == 10664) #this stop_id does not exist according to trimet.org - change this id to 8196
df2 %>% filter(location_id == 8196) #this one is missing too.... 

shape_rs_distinct %>% filter(str_detect(stop_name,"Gateway")) #10864
#we could change all the location 

df2 %>% filter(str_detect(location, "Gateway")) %>% group_by(location) %>% count() %>% arrange(desc(n)) #all but one of these location names is Gateway TC

df2 %>% filter(is.na(lon)) %>% group_by(location_id) %>% count() %>% arrange(desc(n))

df2 %>% filter(lon == -122.5097)
#######


skim(test)
shape_rs_distinct %>% filter(location_id == 10670)
```


Looks like I need to roll up the sleeves and assign an active location_id to as many of these missing rows as possible: 

*note - this data cleaning process could be automated with find/replace function. I did this manually for at least stops with 100 hits or more to ensure that their new location_id was a legitimate and active stop in transit system. To visit. * 

I am changing the `location_id` in `df` to match a location that is recognized in the `shape_rs` spatial data table. I have done this for *all locations with at least 100 incidents*. 

```{r}
#one component of function to automate this process: 
clean_data = function(old,new){
  df = df %>% mutate(location_id = ifelse(location_id == old,new, location_id)) #trying to make life easier by automating this part of find and replace 
  return(df)
}


#location_ids with at least 100 incidents 
missing_df2 = df2 %>% filter(is.na(lon)) %>% group_by(location_id) %>% count() %>% arrange(desc(n)) %>% filter(n > 100)
```


```{r}
#changing location_id of Gateway TC to 10864 in df
df = df %>% mutate(location_id = ifelse(str_detect(location, "Gateway") & !str_detect(location, "[Tt]errace"),10864, location_id)) 

#location_id = 0 
df2 %>% filter(location_id == 0) #4,149 of these simply do not have a location...it may be fruitless to attempt to hunt for a place to pin these on a map :/ it would be guessing...

#10665
df2 %>% filter(location_id == 10665) %>% group_by(location) %>% count() # Gresham TC ! #active id is 10857
df = df %>% mutate(location_id = ifelse(location_id == 10665,10857, location_id))

#10659
df2 %>% filter(location_id == 10659) #Beaverton TC 
shape_rs %>% filter(str_detect(stop_name, "Beaverton")) #9654
df = df %>% mutate(location_id = ifelse(location_id == 10659,9654, location_id))

#10664
df2 %>% filter(location_id == 10664) #Gateway TC
df = df %>% mutate(location_id = ifelse(location_id == 10664,10857, location_id))

#11351
df2 %>% filter(location_id == 11351) #CTC 
shape_rs %>% filter(str_detect(stop_name, "Clackamas")) #13248
df = df %>% mutate(location_id = ifelse(location_id == 11351,13248, location_id))

#10672
df2 %>% filter(location_id == 10672) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Tigard ")) #8210
df = df %>% mutate(location_id = ifelse(location_id == 10672,8210, location_id))

#10660
df2 %>% filter(location_id == 10660) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Hollywood")) %>% select(stop_name, location_id)

df = clean_data(10660,10872)



#8358
df2 %>% filter(location_id == 8358) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Cleveland Ave MAX Station")) %>% select(stop_name, location_id)
df = clean_data(8358,8359)

#11431
df2 %>% filter(location_id == 11431) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Oregon City")) %>% select(stop_name, location_id) # 8760
df = clean_data(11431,8760)

#4392
df2 %>% filter(location_id == 4392) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Parkrose")) %>% select(stop_name, location_id) # 10856
df = clean_data(4392,10856)

the_rest = missing_df2$location_id[11:28]

the_rest_names = for (i in the_rest){
  y = df2 %>% filter(location_id == i) %>% select(location) %>% slice_head(n = 1) %>% pull()
  return(y)
}

the_rest_names <- map(the_rest, ~ df2 %>% filter(location_id == .x) %>% select(location) %>% slice_head(n = 1) %>% pull())

#9847
df2 %>% filter(location_id == 9847) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Hatfield")) %>% select(stop_name, location_id) 

df = clean_data(9847,9848)

#11919
df2 %>% filter(location_id == 11919) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Parkrose")) %>% select(stop_name, location_id) 

df = clean_data(11919,10856)


#11517 
df2 %>% filter(location_id == 11517) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Expo Center")) %>% select(stop_name, location_id)

df = clean_data(11517,11498)

#10670 
df2 %>% filter(location_id == 10670) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Rose Quarter ")) %>% select(stop_name, location_id) 

df = clean_data(10670,11817)

#13719 
df2 %>% filter(location_id == 13719) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Park Ave MAX")) %>% select(stop_name, location_id) 


df = clean_data(13719,13720)

#10666 
df2 %>% filter(location_id == 10666) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Hillsboro")) %>% select(stop_name, location_id) 

df = clean_data(10666,9954)
#11560  
df2 %>% filter(location_id == 11560) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Beaverton")) %>% select(stop_name, location_id) 

df = clean_data(11560,9654)



#9373 

df2 %>% filter(location_id == 9373) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Jantzen Beach")) %>% select(stop_name, location_id) 

df = clean_data(9373,1026)


#6549 
df2 %>% filter(location_id == 6549) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Holladay")) %>% select(stop_name, location_id) #13th not 11th but close 

df = clean_data(6549,8513)

#13131 
df2 %>% filter(location_id == 13131) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clackamas Town Center")) %>% select(stop_name, location_id) 
df = clean_data(13131,13248)


#11632 
df2 %>% filter(location_id == 11632) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Gateway")) %>% select(stop_name, location_id) 
df = clean_data(11632,10864)


#12658
df2 %>% filter(location_id == 12658) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "& Foster")) %>% select(stop_name, location_id) 

df = clean_data(12658,8134)


#10671 
df2 %>% filter(location_id == 10671) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Sunset")) %>% select(stop_name, location_id) 

df = clean_data(10671,9975)

#12452 
df2 %>% filter(location_id == 12452) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Ruby")) %>% select(stop_name, location_id) 

df = clean_data(12452,8355)
#13140 
df2 %>% filter(location_id == 13140) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Jackson")) %>% select(stop_name, location_id) 
df = clean_data(13140,5028)
#13428 
df2 %>% filter(location_id == 13428) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clack")) %>% select(stop_name, location_id) 
df = clean_data(13428,13248)

#12740 
df2 %>% filter(location_id == 12740) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clack")) %>% select(stop_name, location_id) 
df = clean_data(12740,13248)
#10661
df2 %>% filter(location_id == 10661) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Barbur")) %>% select(stop_name, location_id)
df = clean_data(10661,8213)


```


### JOIN of df with shape
```{r}
df2 = df %>% left_join(shape_rs_distinct %>% select(-rte,-dir,-rte_desc,-dir_desc,-stop_seq), by = "location_id")

```

Assessing missing/dirty data in `df2`
```{r}

missing_per_year = df2 %>% filter(is.na(lon)) %>% group_by(year) %>% count() # missing spatial data due to lack of location_id match

total_per_year  = df2 %>% group_by(year) %>% count() 

missing_per_year %>% inner_join(total_per_year, by = "year") %>% mutate(perc_missing = n.x / n.y) # we have between 13% and 29% of incidents missing the right location data. 

#what if we take out the location_id = 0: 
missing_per_year_no_0 = df2 %>% filter(location_id != 0) %>% filter(is.na(lon)) %>% group_by(year) %>% count()
total_per_year_no_0  = df2 %>% filter(location_id != 0) %>% group_by(year) %>% count() 
missing_per_year_no_0 %>% inner_join(total_per_year_no_0, by = "year") %>% mutate(perc_missing = n.x / n.y) #I feel slightly better about 10-20 % removal of data across all years. 

df2 = df2 %>% filter(location_id != 0) %>% filter(!is.na(lon))


skim(df2)
#the cols with NA are comments (427) and route (13210) - not every incident could necessarily be associated with a route, since some incidents happen not on car/train but on trimet property. We can always remove those later. For now I will use this subsetted data to gain insight upon where in the trimet system events happen. 

```

Clean-up time
```{r}
df2[c('type','jurisdic','frequent','namelsadco')] =  lapply(df2 %>% select(type,jurisdic,frequent,namelsadco), factor)
```

Examining cleaned up dataset, `df2`: 
```{r}
df2 %>% group_by(year) %>% count() %>% ggplot(aes(x = year, y = n)) + geom_col()  # it still follows the same trend as the original ds. 

#which zip code has the most incidents? 
df2 %>% group_by(zipcode) %>% count() %>% arrange(desc(n)) # 97220 - that is 82nd to 122nd, North of Burnside up to the Columbia

#which zip code has the least incidents
df2 %>% group_by(zipcode) %>% count() %>% arrange(n) # 97022 - Clackamas county, Eagle Creek 

```
Map of incidents in tri-county area: 

```{r}
ggplot() +
  geom_sf(data = tri_county_tracts, aes(fill = GEOID)) +
  geom_point(data = df2, aes(x = lon, y = lat), color = 'red', alpha = .05, size =.5) + 
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.6)) + 
  theme(legend.position = "none")
 
```
Mapping by tract 

```{r}
df2 %>% ggplot() + geom_sf(mapping = aes(geometry = geometry.y, fill = namelsadco )) + # okay this is good, I have a layer of each tract across the 3 counties
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.6)) # there appear to be some difference between this and map above.... 

# there some tracts that are missing...or that have no data? 
```

Aggregating incident count by tract
```{r}
incident_per_tract = df2 %>% group_by(tractce) %>%
  count() %>%
  ungroup %>%
  left_join(df2 %>% select(tractce, namelsadco, geometry.y), by = "tractce") 

  
ggplot(incident_per_tract) +
  geom_sf(aes(geometry = geometry.y, fill = log(n))) + # logging n to condense the scale to show more variation 
  scale_fill_gradient(
      low = "white",
      high = "darkred",
      na.value = "grey50") +
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.7)) + 
  labs(title = "Trimet Security Incident Hotspots, 2010-2023")





```

It looks like the areas along East (Gresham) -West (Hillsboro) MAX railway have the highest concentrations of of security incidents. 

Identifying top 5 tracts by incident count across data set 
```{r}
top5tracts = df2 %>% group_by(tractce) %>% count() %>% arrange(desc(n)) %>% head(n = 5) 

df2 %>% filter(tractce %in% top5tracts$tractce) %>% group_by(location) %>% count() %>% arrange(desc(n))

head(df2)
                                                           
```

Gateway Transit Center (008100)
Rose Quarter Transit Center (002303)
Beaverton Transit Center (031302)
Cleveland Ave Max Station (010001)
Gresham Transit Center (010001)
Clackamas Town Center (022201)
Downtown Transit Mall (010602)

This makes sense....as these TriMet properties likely have the highest volume across the system. 

What about non-MAX incidents? 
```{r}
df2 %>% filter(type != 'MAX') %>% group_by(tractce) %>% count() %>% arrange(desc(n))

# 008100 = gateway transit center area
```
Loading wrangled datasets, `df3`: 
```{r}
df3 = read_rds('https://raw.githubusercontent.com/karolo89/capstone/main/data/df_spatial_clean.rds')
```



Let's map the incident activity in 2022 in tracte 008100: 

```{r}
gateway_2022 = df3 %>% filter(tractce == '008100' & year == 2022) %>% group_by(location_id) %>% count() 
gateway_2022 = inner_join(gateway_2022, df3 %>% select(location_id, lon, lat) %>% distinct(), by = 'location_id')

ggplot() +
  geom_sf(data = tri_county_tracts %>% filter(TRACTCE == '008100')) +
  geom_point(data = gateway_2022, aes(x = lon, y = lat, size =n), color = 'red') + 
  theme(legend.position = "none")

#we need a street level map for to show where these events are. 
```


```{r}
metro_2022 = df3 %>% filter(year == 2022) %>% group_by(location_id) %>% summarize(n = n())
metro_2022 = inner_join(metro_2022, df3 %>% select(location_id, lon, lat) %>% distinct(), by = 'location_id')

ggplot() +
  geom_sf(data = tri_county_tracts) +
  geom_point(data = metro_2022, aes(x = lon, y = lat, size = n, color = n, alpha = n)) + 
  theme(legend.position = "none") + 
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.7)) + 
  scale_color_gradient(low = 'pink', high = 'darkred') + 
  labs(title = "Number of security incidents in 2022, by location_id")

metro_2022 %>% arrange(desc(n)) #10864 had 350 incidents, by far the most 

df3 %>% filter(location_id == "10864") %>% head(n = 1) %>% select(location) # Gateway Transit Center 
```

Gateway Transit center is by far the place with most security incidents in 2022. 

What stops are the other 4 stops with the most security incidents in 2022? 
```{r}
top5_2022 = metro_2022 %>% arrange(desc(n)) %>% head(n =5) #10864 had 350 incidents, by far the most 

inner_join(top5_2022,df3 %>% select(location_id,location), by = 'location_id') %>% group_by(location_id) %>% distinct(location) %>% tail(n =5)
```
The other stops with the most security incidents are: 

Elmonica / SW 170th Ave MAX Station 
Hollywood/NE 42nd Ave TC 
Beaverton TC MAX Station 
Rose Quarter TC MAX Station

Note that these stops are not necessarily the most dangerous. They have the highest volume of traffic and incidents. It does make sense that the MAX would have more security incidents, as they hold more people, often have less supervision on the cars, and they offer shelter / seclusion at times. It will be interesting to see what _types_ of incidents occur the most at these sites. 


Is this trend for Gateway TC true for other years? Does it match the overall trend in the system in the same time frame? 

```{r}
gateway_tc = df3 %>% filter(location_id == 10864)
gateway_tc %>% group_by(year) %>% count() %>% ggplot() + geom_col(aes(x = year, y = n)) # this is a familiar pattern...

df3 = df3 %>% mutate(is_gateway = ifelse(location_id == 10864, 1,0))

df3 %>% group_by(year, is_gateway) %>% count() %>% ggplot() + geom_col(aes(x = year, y = n, fill = factor(is_gateway, levels = c(0,1)))) + 
  guides(fill = guide_legend(title = "Is Gateway TC"))


```

```{r}
gateway_year_counts = gateway_tc  %>% group_by(year) %>% count()       
df3_year_counts = df3 %>% filter(is_gateway == 0) %>% group_by(year) %>% count()

cor(gateway_year_counts$n,df3_year_counts$n) #the increase in security incidents across metro area is highly correlated with the increase in incidents at Gateway
```


Looking at hotspots in 2021 vs. 2022 
```{r}
library(plotly)

incident_per_tract_2022= df3 %>% filter(year == 2022) %>% group_by(tractce) %>%
  count() %>% left_join(df3 %>% select(tractce, namelsadco, geometry.y), by = "tractce") 

incident_per_tract_2021= df3 %>% filter(year == 2021) %>% group_by(tractce) %>%
  count() %>% left_join(df3 %>% select(tractce, namelsadco, geometry.y), by = "tractce") 

choropleth = function(subset){
ggplot(subset) +
  geom_sf(aes(geometry = geometry.y, fill = log(n))) + # logging n to condense the scale to show more variation 
  scale_fill_gradient(
      low = "white",
      high = "darkred",
      na.value = "grey50") +
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.7)) + 
  labs(title = "Trimet Security Incident Hotspots, 2022") 
}

choropleth(incident_per_tract_2021)
choropleth(incident_per_tract_2022)



```

Animated choropleth across years 2010 - 2023
```{r}
library(plotly)

incident_per_tract = df3 %>% group_by(tractce) %>%
  count() %>% left_join(df3 %>% select(tractce,year, namelsadco, geometry.y), by = "tractce") 

plot = ggplot(incident_per_tract, aes(frame = year)) +
  geom_sf(aes(geometry = geometry.y, fill = log(n))) + # logging n to condense the scale to show more variation 
  scale_fill_gradient(
      low = "white",
      high = "darkred",
      na.value = "grey50") +
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.7)) + 
  labs(title = "Trimet Security Incident Hotspots") 

ggplotly(plot)
```

This animation does not play smoothly, however it is apparent that the same tracts have similarly high concentrations of incidents across these 13 years.  


In 2022, which tracts had more than 100 security incidents in that year ? 

```{r}
top15_tracts_2022 = incident_per_tract_2022 %>% filter(n >= 100) %>% distinct(tractce) %>% inner_join(df3 %>% filter(year == 2022), top15_tracts_2022, by = "tractce") 

#let's plot these events on the map 
ggplot() +
  geom_sf(data = tri_county_tracts) +
  geom_point(data = top15_tracts_2022 %>% group_by(location_id) %>% summarize(n = n(), lon, lat) %>% distinct(n, lon, lat), 
             aes(x = lon, y = lat, size = n), color = 'blue', alpha = .5) +
  coord_sf(xlim = c(-122.85, -122.4), ylim = c(45.425, 45.6)) + 
  theme(legend.position = "none") 

#this basically shows where each max line stops.
```

Top 15 tracts 
```{r}
unique(top15_tracts_2022$tractce)
```

Which routes are most involved in these areas with high incident counts? It appears that it is the Red and Blue MAX lines. 


Within these tracts, if we take out the MAX related security incidents, are they still high-incident areas?     
```{r}
df3 %>% filter(tractce %in% top15_tracts_2022$tractce & year == 2022 & type != 'MAX') %>% group_by(tractce) %>% count()

df3 %>% filter(tractce %in% top15_tracts_2022$tractce & year == 2022 ) %>% group_by(tractce) %>% count()
```
Most of the tracts are not that bad when you take out Max related incidents, except Gateway transit center area....






## TIME ANALYSIS 
```{r}
# Load necessary libraries
library(ggplot2)
library(gganimate)
library(dplyr)

# Group data by year and location, and count the number of incidents
df_grouped <- df3 %>%
  group_by(year, location) %>%
  summarise(incident_count = n())

# Create ggplot object
p <- ggplot(df_grouped, aes(x = year, y = incident_count, color = location)) +
  geom_line(size = 1) +
  geom_point(data = subset(df_grouped, year == 2022), colour = "red", size = 4, shape = 4) +
  labs(
    title = "Change in Number of Incidents Over Time",
    x = "Year",
    y = "Number of Incidents",
    color = "Location"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        legend.position = "none") +
  geom_text(
    data = subset(df_grouped, year == 2022),
    aes(label = ifelse(year == 2022, "???", "")),
    vjust = -1.5,
    hjust = 0.5,
    size = 4,
    colour = "red"
  )

```

```{r}
# Add animation
p_animate <- p + transition_reveal(year)

# Render the animation
animate(p_animate, end_pause = 50, height = 400, width = 600) # error: `geom_line()`: Each group consists of only one observation.

```

Clearly significant results in the year 2022. This will be the year we focus on to prime our analysis. Lets look to see if there is a trend with specific seasons/months.

```{r}
# Filter original dataframe to include only rows from the year 2022
df_2022 <- df3 %>%
  filter(year == 2022)

# Assign a 'season' column to df_2022
df_2022 <- df_2022 %>%
  mutate(season = case_when(
    month %in% c(12, 1, 2) ~ "Winter",
    month %in% 3:5 ~ "Spring",
    month %in% 6:8 ~ "Summer",
    month %in% 9:11 ~ "Autumn",
    TRUE ~ as.character(month)
  ))


# Assign a 'season' column to df_2022 based on 'month'
df_2022 <- df_2022 %>%
  mutate(season = case_when(
    month %in% c("Dec", "Jan", "Feb") ~ "Winter",
    month %in% c("Mar", "Apr", "May") ~ "Spring",
    month %in% c("Jun", "Jul", "Aug") ~ "Summer",
    month %in% c("Sep", "Oct", "Nov") ~ "Autumn",
    TRUE ~ as.character(month)
  ))

```



```{r}
library(lubridate)

# Calculate the number of incidents per month
df_sorted_2022 <- df_2022 %>%
   group_by(month, season) %>%
   summarise(incident_count = n(), .groups = "drop") %>%
   arrange(month)

# Calculate the average number of incidents per month
df_avg_2022 <- df_2022 %>%
  group_by(month) %>%
  summarise(avg_incident_count = n(), .groups = "drop") %>%
  arrange(month)

# Plotting the bar plot with the trendline
# Define a color palette
color_palette <- c("Winter" = "#1f78b4", "Spring" = "#33a02c", "Summer" = "#e31a1c", "Autumn" = "#ff7f00")

ggplot(df_sorted_2022, aes(x = month, y = incident_count, fill = season)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = color_palette) +
  labs(title = "Number of Incidents by Month in 2022", x = "Month", y = "Number of Incidents", fill = "Season") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "bottom") +
  geom_line(data = df_avg_2022, aes(x = month, y = avg_incident_count, group = 1), 
            color = "black", size = 1, inherit.aes = FALSE)

```

Appears to be a trend that in the warmer seasons incidents increase. Lets see if the statistics support this with a chi squared test of proportional independence.

```{r}

# Chi-square test for independence between month and incidents
chisq_test_month <- chisq.test(table(df_2022$month))

print(chisq_test_month)

# Chi-square test for independence between season and incidents
chisq_test_season <- chisq.test(table(df_2022$season))

print(chisq_test_season)

```


```{r}
# Extract standardized residuals
stdres <- chisq_test_month$stdres
names(stdres) <- month.abb

# Print residuals
print(stdres)

# Highlight residuals greater than 2 in absolute value
significant_months <- names(stdres)[abs(stdres) > 2]

print(paste("Months with significantly different number of incidents than expected: ", paste(significant_months, collapse = ", ")))

```

The following months show statistical significance, specifically in that April, September, and October have less than expected incidents while Jan, & May-July have more than expected incidents. Lets see if there are trends within time of day.


## TEXT DATA ANALYSIS 
```{r}

```

