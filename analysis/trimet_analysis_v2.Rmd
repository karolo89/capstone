---
title: "Trimet Security Analysis"
author: Karol Orozco, Corey Cassell, Justus Eaglesmith, Charles Hanks, & CorDarryl Hall
output: html_document
date: "2023-06-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(lubridate)
library(gridExtra)
library(forcats)
library(stringr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(tigris)
library(sf)
library(skimr)
```

## DATA CLEANING

```{r}
#this is the raw data provided by TriMet, through Karol, on Jun 24 2023
df = read_csv(
  'https://raw.githubusercontent.com/karolo89/capstone/main/data/trimet_2010_2023.csv') %>%
  rename_all(funs(tolower(.)))   %>% # I like lower case columns
  select(-incident_begin_date, # removing non-pertinent columns 
         -division_code,
         -x_coordinate,
         -y_coordinate,
         -loc_x,
         -loc_y) 

#adding date + time columns:
df = df %>% mutate(date = as.POSIXct(date, format = "%m/%d/%Y")) %>% 
  mutate(
    year = year(date),
    month = month(date, label = TRUE),
    day = day(date),
    wday = wday(date, label = TRUE),
    hour = hour(time))

#looks like we have some duplicate incidents 
df= df[!duplicated(df$incident_id),]

#quick scrub of some unwanted tags in comments 
patterns = c("<Notification>","\r","\n")
df = df %>% mutate(comments = gsub(paste(patterns, collapse = "|"), "", comments))

#looks like several columns are incomplete, may have to make some sacrifices for the sake of clean, workable data. 
#For example, I think for our purposes we can drop garage, primary_vehicle_flag, and train.

df = df %>% select(-garage, -primary_vehicle_flag, -train)

df %>% filter(is.na(route_number)) # note that 18,422 of the incidents do not have a route number assigned, this may be consideration later in analysis when drilling down to incidents per route 

# every incident has a location id! This is good. 
df %>% filter(!is.na(location_id)) 

skim(df) #also note: 524 rows are missing comments, 4149 rows are missing location names (this coudl be rectified as we have location_id)


```

## LOCATION DATA ANALYSIS 

### Loading TriMet Shapefiles

routes & stops (primary shapefile)
```{r}
#loading trimet's 'routes & stops' shapfiles, courtesy of Trimet's developer resources: <https://developer.trimet.org/>
shape_rs = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_route_stops', 
                layer = 'tm_route_stops') #loading routes and stops shapefile data 

# confirming that shapefile `stop_id` is the same as security dataset's `location_id` 
shape_rs %>% filter(str_detect(stop_name,"Hollywood")) # oneHollywood TC id is 10871

```

Additional shapefiles
```{r}
shape_rail_stops = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_rail_stops', 
                layer = 'tm_rail_stops') 

coords_rail_stops = st_transform(shape_rail_stops,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_rail_stops = bind_cols(shape_rail_stops, coords_rail_stops) %>% rename(lat = Y, lon = X)

#nope, this does not have the data I want 


shape_parkride = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_parkride', 
                layer = 'tm_parkride') 

coords_parkride = st_transform(shape_parkride,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_parkride = bind_cols(shape_parkride, coords_parkride) %>% rename(lat = Y, lon = X)

#nice to have, but this does not have location_id 

shape_tc = read_sf(dsn = '/Users/chanks/workspace/capstone/data/shapefiles/tm_tran_cen', 
                layer = 'tm_tran_cen')
```


Preparing tract data
```{r}
library(tigris)
options(tigris_use_cache = TRUE)

this.year = 2020

mult_tracts = tracts(state = 'OR', county = "Multnomah", cb = T, year = this.year)
clack_tracts = tracts(state = 'OR', county = "Clackamas", cb = T, year = this.year)
wash_tracts = tracts(state = 'OR', county = "Washington", cb = T, year = this.year)

tri_county_tracts = bind_rows(mult_tracts,clack_tracts,wash_tracts)

plot(tri_county_tracts)
```

Adding WGS84 coordinates to routes and stops shapefile 
```{r}
coords_rs = st_transform(shape_rs,"+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  st_coordinates()

shape_rs = bind_cols(shape_rs, coords_rs) %>% mutate(lat = Y, lon = X) %>% select(-Y,-X) #rename(lat = Y, lon = X)
# I have confirmed on https://www.gps-coordinates.net/ that these lat + lon coords correspond to stop / intersection 

length(unique(shape_rs$stop_id)) #6430 distinct stop_ids 
length(shape_rs$stop_id) # 8539 total stop_ids listed, this is likely because multiple routes share the same stop

#shape_rs %>% group_by(stop_id) %>% count() %>% arrange(desc(n))
# ther are 21 routes that use stop id 13248

shape_rs = st_transform(shape_rs, crs = st_crs(tri_county_tracts)) #joining tract information to shape_rs (for mapping later)
shape_rs = st_join(shape_rs,tri_county_tracts)  

#I also want the tract multipolygon spatial data
class(tri_county_tracts) # this is a sf object, I need to cast it as data frame to do non-spatial join 

shape_rs = shape_rs %>% left_join(tri_county_tracts %>% select(TRACTCE,geometry) %>% as.data.frame(), by = "TRACTCE")


shape_rs = shape_rs %>% rename_all(funs(tolower(.))) 
```


Prepping route and stops shapefile to join 
```{r}
#changing stop_id to location_id so that dataframes may join on common key (rename() throws an error that I haven't resolved)
shape_rs = shape_rs %>% mutate(location_id = stop_id) %>% select(-stop_id) %>% relocate(location_id, .after = stop_name)

#selecting only the relevant columns to join: 
#shape_rs = shape_rs %>% select(stop_name, location_id, zipcode, frequent, geometry, lon, lat) 

#we only want one location_id (formerly stop_id) in this shape file to avoid a many-to-many join ("For stops served by multiple lines there are multiple records in this dataset.")
shape_rs_distinct = shape_rs %>% distinct(location_id, .keep_all = TRUE)

shape_rs_distinct %>% filter(location_id == 13248) # confirming that this clackamas tc stop has only one row



```


The `shape_rs` table is missing some locations that are in the `df2`....I will hunt these down.
```{r}
#for example, this gateway transit stop 
df2 %>% filter(location_id == 10664) #this stop_id does not exist according to trimet.org - change this id to 8196
df2 %>% filter(location_id == 8196) #this one is missing too.... 

shape_rs_distinct %>% filter(str_detect(stop_name,"Gateway")) #10864
#we could change all the location 

df2 %>% filter(str_detect(location, "Gateway")) %>% group_by(location) %>% count() %>% arrange(desc(n)) #all but one of these location names is Gateway TC

df2 %>% filter(is.na(lon)) %>% group_by(location_id) %>% count() %>% arrange(desc(n))

df2 %>% filter(lon == -122.5097)
#######


skim(test)
shape_rs_distinct %>% filter(location_id == 10670)
```

### JOIN of df with shape_rs_distinct
```{r}
df2 = df %>% left_join(shape_rs_distinct %>% select(-rte,-dir,-rte_desc,-dir_desc,-stop_seq), by = "location_id")

```
Looks like I need to roll up the sleeves and assign an active location_id to as many of these missing rows as possible: 

*note - this data cleaning process could be automated with find/replace function. I did this manually for at least stops with 100 hits or more to ensure that their new location_id was a legitimate and active stop in transit system. To visit. * 

I am changing the `location_id` in `df` to match a location that is recognized in the `shape_rs` spatial data table. I have done this for *all locations with at least 100 incidents*. 

```{r}
#one component of function to automate this process: 
clean_data = function(old,new){
  df = df %>% mutate(location_id = ifelse(location_id == old,new, location_id)) #trying to make life easier by automating this part of find and replace 
  return(df)
}


#location_ids with at least 100 incidents 
missing_df2 = df2 %>% filter(is.na(lon)) %>% group_by(location_id) %>% count() %>% arrange(desc(n)) %>% filter(n > 100)
```


```{r}
#changing location_id of Gateway TC to 10864 in df
df = df %>% mutate(location_id = ifelse(str_detect(location, "Gateway") & !str_detect(location, "[Tt]errace"),10864, location_id)) 

#location_id = 0 
df2 %>% filter(location_id == 0) #4,149 of these simply do not have a location...it may be fruitless to attempt to hunt for a place to pin these on a map :/ it would be guessing...

#10665
df2 %>% filter(location_id == 10665) %>% group_by(location) %>% count() # Gresham TC ! #active id is 10857
df = df %>% mutate(location_id = ifelse(location_id == 10665,10857, location_id))

#10659
df2 %>% filter(location_id == 10659) #Beaverton TC 
shape_rs %>% filter(str_detect(stop_name, "Beaverton")) #9654
df = df %>% mutate(location_id = ifelse(location_id == 10659,9654, location_id))

#10664
df2 %>% filter(location_id == 10664) #Gateway TC
df = df %>% mutate(location_id = ifelse(location_id == 10664,10857, location_id))

#11351
df2 %>% filter(location_id == 11351) #CTC 
shape_rs %>% filter(str_detect(stop_name, "Clackamas")) #13248
df = df %>% mutate(location_id = ifelse(location_id == 11351,13248, location_id))

#10672
df2 %>% filter(location_id == 10672) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Tigard ")) #8210
df = df %>% mutate(location_id = ifelse(location_id == 10672,8210, location_id))

#10660
df2 %>% filter(location_id == 10660) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Hollywood")) %>% select(stop_name, location_id)

df = clean_data(10660,10872)



#8358
df2 %>% filter(location_id == 8358) %>% select(location)
shape_rs %>% filter(str_detect(stop_name, "Cleveland Ave MAX Station")) %>% select(stop_name, location_id)
df = clean_data(8358,8359)

#11431
df2 %>% filter(location_id == 11431) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Oregon City")) %>% select(stop_name, location_id) # 8760
df = clean_data(11431,8760)

#4392
df2 %>% filter(location_id == 4392) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Parkrose")) %>% select(stop_name, location_id) # 10856
df = clean_data(4392,10856)

the_rest = missing_df2$location_id[11:28]

the_rest_names = for (i in the_rest){
  y = df2 %>% filter(location_id == i) %>% select(location) %>% slice_head(n = 1) %>% pull()
  return(y)
}

the_rest_names <- map(the_rest, ~ df2 %>% filter(location_id == .x) %>% select(location) %>% slice_head(n = 1) %>% pull())

#9847
df2 %>% filter(location_id == 9847) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Hatfield")) %>% select(stop_name, location_id) 

df = clean_data(9847,9848)

#11919
df2 %>% filter(location_id == 11919) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Parkrose")) %>% select(stop_name, location_id) 

df = clean_data(11919,10856)


#11517 
df2 %>% filter(location_id == 11517) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Expo Center")) %>% select(stop_name, location_id)

df = clean_data(11517,11498)

#10670 
df2 %>% filter(location_id == 10670) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Rose Quarter ")) %>% select(stop_name, location_id) 

df = clean_data(10670,11817)

#13719 
df2 %>% filter(location_id == 13719) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Park Ave MAX")) %>% select(stop_name, location_id) 


df = clean_data(13719,13720)

#10666 
df2 %>% filter(location_id == 10666) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Hillsboro")) %>% select(stop_name, location_id) 

df = clean_data(10666,9954)
#11560  
df2 %>% filter(location_id == 11560) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Beaverton")) %>% select(stop_name, location_id) 

df = clean_data(11560,9654)



#9373 

df2 %>% filter(location_id == 9373) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Jantzen Beach")) %>% select(stop_name, location_id) 

df = clean_data(9373,1026)


#6549 
df2 %>% filter(location_id == 6549) %>% select(location) %>% slice_head(n = 1) %>% pull() 
shape_rs %>% filter(str_detect(stop_name, "Holladay")) %>% select(stop_name, location_id) #13th not 11th but close 

df = clean_data(6549,8513)

#13131 
df2 %>% filter(location_id == 13131) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clackamas Town Center")) %>% select(stop_name, location_id) 
df = clean_data(13131,13248)


#11632 
df2 %>% filter(location_id == 11632) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Gateway")) %>% select(stop_name, location_id) 
df = clean_data(11632,10864)


#12658
df2 %>% filter(location_id == 12658) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "& Foster")) %>% select(stop_name, location_id) 

df = clean_data(12658,8134)


#10671 
df2 %>% filter(location_id == 10671) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Sunset")) %>% select(stop_name, location_id) 

df = clean_data(10671,9975)

#12452 
df2 %>% filter(location_id == 12452) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Ruby")) %>% select(stop_name, location_id) 

df = clean_data(12452,8355)
#13140 
df2 %>% filter(location_id == 13140) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Jackson")) %>% select(stop_name, location_id) 
df = clean_data(13140,5028)
#13428 
df2 %>% filter(location_id == 13428) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clack")) %>% select(stop_name, location_id) 
df = clean_data(13428,13248)

#12740 
df2 %>% filter(location_id == 12740) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Clack")) %>% select(stop_name, location_id) 
df = clean_data(12740,13248)
#10661
df2 %>% filter(location_id == 10661) %>% select(location) %>% slice_head(n = 1) %>% pull()
shape_rs %>% filter(str_detect(stop_name, "Barbur")) %>% select(stop_name, location_id)
df = clean_data(10661,8213)


```


### JOIN of df with shape
```{r}
df2 = df %>% left_join(shape_rs_distinct %>% select(-rte,-dir,-rte_desc,-dir_desc,-stop_seq), by = "location_id")

```

Assessing missing/dirty data in `df2`
```{r}

missing_per_year = df2 %>% filter(is.na(lon)) %>% group_by(year) %>% count() # missing spatial data due to lack of location_id match

total_per_year  = df2 %>% group_by(year) %>% count() 

missing_per_year %>% inner_join(total_per_year, by = "year") %>% mutate(perc_missing = n.x / n.y) # we have between 13% and 29% of incidents missing the right location data. 

#what if we take out the location_id = 0: 
missing_per_year_no_0 = df2 %>% filter(location_id != 0) %>% filter(is.na(lon)) %>% group_by(year) %>% count()
total_per_year_no_0  = df2 %>% filter(location_id != 0) %>% group_by(year) %>% count() 
missing_per_year_no_0 %>% inner_join(total_per_year_no_0, by = "year") %>% mutate(perc_missing = n.x / n.y) #I feel slightly better about 10-20 % removal of data across all years. 

df2 = df2 %>% filter(location_id != 0) %>% filter(!is.na(lon))


skim(df2)
#the cols with NA are comments (427) and route (13210) - not every incident could necessarily be associated with a route, since some incidents happen not on car/train but on trimet property. We can always remove those later. For now I will use this subsetted data to gain insight upon where in the trimet system events happen. 

```

Clean-up time
```{r}
df2[c('type','jurisdic','frequent','namelsadco')] =  lapply(df2 %>% select(type,jurisdic,frequent,namelsadco), factor)
```

Examining cleaned up dataset, `df2`: 
```{r}
df2 %>% group_by(year) %>% count() %>% ggplot(aes(x = year, y = n)) + geom_col()  # it still follows the same trend as the original ds. 

#which zip code has the most incidents? 
df2 %>% group_by(zipcode) %>% count() %>% arrange(desc(n)) # 97220 - that is 82nd to 122nd, North of Burnside up to the Columbia

#which zip code has the least incidents
df2 %>% group_by(zipcode) %>% count() %>% arrange(n) # 97022 - Clackamas county, Eagle Creek 

```
Map of incidents in tri-county area: 

```{r}
ggplot() +
  geom_sf(data = tri_county_tracts) +
  geom_point(data = df2, aes(x = lon, y = lat), color = 'red', alpha = .05, size =.5) + 
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.6))
 
```
Mapping by tract 

```{r}
df2 %>% ggplot() + geom_sf(mapping = aes(geometry = geometry.y, fill = namelsadco )) + # okay this is good, I have a layer of each tract across the 3 counties
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.6)) # there appear to be some difference between this and map above.... 

# there some tracts that are missing...or that have no data? 
```

Aggregating incident count by tract
```{r}
incident_per_tract = df2 %>% group_by(tractce) %>%
  count() %>%
  ungroup %>%
  left_join(df2 %>% select(tractce, namelsadco, geometry.y), by = "tractce") 

  
ggplot(incident_per_tract) +
  geom_sf(aes(geometry = geometry.y, fill = log(n))) + # logging n to condense the scale to show more variation 
  scale_fill_gradient(
      low = "white",
      high = "darkred",
      na.value = "grey50") +
  coord_sf(xlim = c(-123.1, -122.4), ylim = c(45.3, 45.7)) + 
  labs(title = "Trimet Security Incident Hotspots, 2010-2023")



```

It looks like the areas along East (Gresham) -West (Hillsboro) MAX railway have the highest concentrations of of security incidents. 

Identifying top 5 tracts by incident count across data set 
```{r}
top5tracts = df2 %>% group_by(tractce) %>% count() %>% arrange(desc(n)) %>% head(n = 5) 

df2 %>% filter(tractce %in% top5tracts$tractce) %>% group_by(location) %>% count() %>% arrange(desc(n))
                                                           
```

Gateway Transit Center (008100)
Rose Quarter Transit Center (002303)
Beaverton Transit Center (031302)
Cleveland Ave Max Station (010001)
Gresham Transit Center (010001)
Clackamas Town Center (022201)
Downtown Transit Mall (010602)

This makes sense....as these TriMet properties likely have the highest volume across the system. 

What about non-MAX incidents? 
```{r}
df2 %>% filter(type != 'MAX') %>% group_by(tractce) %>% count() %>% arrange(desc(n))

```

Exporting wrangled dataset: 
```{r}
write_rds(df2, file = 'df_spatial_clean.rds')
```


Loading wrangled datasets, `df3`: 
```{r}
df3 = read_rds('https://raw.githubusercontent.com/karolo89/capstone/main/data/df_spatial_clean.rds')
```




## TEMPORAL DATA ANALYSIS 
```{r}

```



## TEXT DATA ANALYSIS 
```{r}

```

